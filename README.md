AI Motion Radar (YOLOv8 Object Tracking & Speed Estimation)

A computer vision project that detects, tracks, and estimates the speed of moving road users from traffic footage.
I annotated my own dataset (484 images) in CVAT, exported in YOLO format, trained a custom YOLOv8 model on Google Colab, tested locally in VSCode, and deployed with Streamlit Community Cloud.

The model is trained to recognize 5 traffic classes:

ğŸš— Car

ğŸšŒ Bus

ğŸ›º Tricycle (Keke)

ğŸš¶ Person

ğŸï¸ Bike

âœ¨ Features

Upload a video (MP4/AVI/MOV/MKV)

YOLOv8 detection + tracking (ByteTrack) â†’ stable IDs

Per-object speed estimate (m/s) using frame-to-frame displacement

Supports 5 custom classes: car, bus, tricycle, person, bike

Works on CPU (no GPU required)

One-click deployment on Streamlit Cloud

âš ï¸ Note: Speed is an estimate. Accuracy depends on camera angle, FPS, and your calibration (pixels_per_meter).


ğŸ§  How it Works

Detect objects in each frame with my trained YOLOv8 model (best.pt).

Track detections across frames with ByteTrack so each object keeps a stable ID.

For every object ID, compute the distance moved between this frame and the previous frame (in pixels).

Convert pixels â†’ meters using a simple scale: pixels_per_meter (set in the sidebar).

Convert per-frame distance to meters/second using the video FPS.

Draw bounding boxes with class, ID, and estimated speed.

Speed formula used:


ğŸ“‚ Dataset

Annotated 484 traffic frames in CVAT

Classes: car, bus, tricycle, person, bike

Exported in YOLO format (images + TXT labels)

~80/20 split for train/val


ğŸ“¦ Project Structure

capstonesfai_project
â”œâ”€â”€ streamlit_app.py       # Streamlit UI + processing loop (tracking + speed)
â”œâ”€â”€ best.pt                # Trained YOLOv8 weights (custom 5-class model)
â”œâ”€â”€ requirements.txt       # Python dependencies (for local/Cloud)
â”œâ”€â”€ packages.txt           # System dependencies for Cloud (ffmpeg, libGL)
â””â”€â”€ README.md  


ğŸš€ Quick Start (Local)

Requirements: Python 3.9â€“3.11 recommended.
# 1) Create and activate virtual environment (Windows PowerShell)
python -m venv venv
./venv/Scripts/Activate

# 2) Install dependencies
pip install -r requirements.txt

# 3) Run the app
streamlit run streamlit_app.py


â˜ï¸ Deploy to Streamlit Community Cloud

Push this project to a public GitHub repo (include best.pt).

Go to https://share.streamlit.io â†’ New app.

Select repo/branch and entrypoint â†’ streamlit_app.py.

Streamlit Cloud auto-installs packages and runs the app.

Your app will be live at:
https://<username>-ai-motion-radar.streamlit.app


ğŸ§© Key Files
requirements.txt

CPU-friendly dependencies:
streamlit
ultralytics
opencv-python-headless
numpy
pillow
matplotlib
imageio
imageio-ffmpeg

--extra-index-url https://download.pytorch.org/whl/cpu
torch
torchvision


packages.txt

System dependencies:
ffmpeg
libgl1
libglib2.0-0

ğŸ§ª Training Process
1) Annotation (CVAT)

Imported traffic video â†’ extracted frames

Labeled car, bus, tricycle, person, bike

Exported in YOLO format

2) Training (Colab)

!pip install ultralytics
from ultralytics import YOLO

model = YOLO("yolov8n.pt")
model.train(data="/content/data.yaml", epochs=50, imgsz=640)


3) Deployment

Downloaded best.pt

Integrated into Streamlit app

Deployed to Streamlit Cloud


ğŸ§© App Highlights

YOLO("best.pt") â†’ loads trained model

model.track(..., persist=True, tracker="bytetrack.yaml") â†’ stable IDs

Per-object speed estimated using bounding box centers


ğŸ“Š Tips for Calibration

Use a known real-world distance (lane width â‰ˆ 3.5m, zebra crossing, etc.)

Count its pixels in the frame

pixels_per_meter = pixels / meters

Use a region at similar depth to where motion is measured


âœ… Results

Detects cars, buses, tricycles, persons, bikes in real-time

Tracks objects with unique IDs

Provides estimated speed overlay



ğŸ‘¨â€ğŸ’» Author

Developed by Abimbola Alimat
Capstone Project â€” AI Motion Radar